
# ğŸ›’ Ecommerce Data Pipeline and Analysis

This project implements an end-to-end data pipeline to process, transform, and analyze e-commerce transaction data. It automates the ingestion of synthetic data into PostgreSQL, transfers it to BigQuery through GCS, performs multi-layered transformations using dbt, and visualizes insights via Looker Studio. The pipeline runs using Apache Airflow and Docker, with support for email notifications on failure.

---

## ğŸ“ Configuration & Secrets

- GCP Service Account JSON: `config/key.json`
- PostgreSQL credentials, GCP project ID, and SMTP settings should be stored as environment variables or managed via Airflow Variables.
- Raw ingested data is stored by date in:  
  `data/source/csv/YYYY-MM-DD/`

---

## ğŸ¯ Project Objectives

- â±ï¸ Automate ETL process using **Airflow**
- ğŸ Ingest synthetic e-commerce data into **PostgreSQL**
- â˜ï¸ Extract & load PostgreSQL data into **GCS**, then into **BigQuery**
- ğŸ”„ Transform datasets using **dbt** across staging, core, and datamart layers
- ğŸ“Š Create business dashboards in **Looker Studio**
- ğŸ“§ Implement real-time email alerts using **SMTP**

---

## âš™ï¸ Tech Stack

| Category            | Tools                                     |
|---------------------|-------------------------------------------|
| Orchestration        | Apache Airflow (Docker)                  |
| Database             | PostgreSQL                               |
| Data Lake            | Google Cloud Storage (GCS)               |
| Data Warehouse       | Google BigQuery                          |
| Transformation       | dbt (Data Build Tool)                    |
| Programming Language | Python                                   |
| Visualization        | Looker Studio                            |
| Notification         | SMTP via Airflow                         |
| Containerization     | Docker & Docker Compose                  |

---

## ğŸ—ï¸ Project Structure

```bash
â”œâ”€â”€ airflow/                      
â”‚   â”œâ”€â”€ dags/                     # All Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ ingest_to_postgres.py
â”‚   â”‚   â”œâ”€â”€ postgres_to_gcs.py
â”‚   â”‚   â”œâ”€â”€ gcs_to_bq_staging.py
â”‚   â”‚   â”œâ”€â”€ bq_transform_core.py
â”‚   â”‚   â””â”€â”€ bq_transform_datamart.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dbt/                          
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ dim/
â”‚   â”‚   â”œâ”€â”€ fact/
â”‚   â”‚   â””â”€â”€ datamart/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ main_ingestion.py         # Python script to insert dummy data into PostgreSQL
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ key.json                  # GCP service account key
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ source/csv/YYYY-MM-DD/
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Run the Project

### ğŸ“¦ 1. Start Docker + Airflow

```bash
docker-compose build
docker-compose up -d
```

Visit the Airflow web UI: [http://localhost:8080](http://localhost:8080)  
(default login: `airflow` / `airflow`)

---

### ğŸ 2. Ingest Synthetic Data (to PostgreSQL)

```bash
cd ingestion/
python main_ingestion.py
```

- This script creates dummy transaction data using `Faker` and inserts it into PostgreSQL.
- You can modify the number of inserted rows via `main(n=1000)`.

---

### â³ 3. Trigger Airflow DAGs (in order)

In Airflow UI, manually trigger the following DAGs sequentially:

1. `ingest_to_postgres` (optional if you run the script manually)
2. `postgres_to_gcs`
3. `gcs_to_bq_staging`
4. `bq_transform_core`
5. `bq_transform_datamart`

---

### ğŸ§  4. Run dbt Manually (optional via CLI)

```bash
cd dbt/
dbt run
```

You can also run `dbt test` to validate model logic.

---

### ğŸ“Š 5. View Dashboard

Open your [Looker Studio](https://lookerstudio.google.com/) dashboard (linked manually).

Metrics visualized:

- Total Sales & Revenue Trends
- Top Selling Products
- Customer Transaction Behavior
- Payment Preferences
- Channel & Store Performance

---

### ğŸ“§ 6. Email Notification

If any Airflow task fails, you will receive an email with:

- DAG and task names
- Error message and exception
- Execution timestamp
- Log URL (Airflow UI)

Configured via Airflow callbacks with SMTP (`send_email_smtp`).

---

## ğŸ§± Data Warehouse Layers

### ğŸŸ¨ Staging Layer (`stg_*`)
Raw structured data from GCS â†’ BigQuery.

### ğŸŸ¦ Core Layer (Fact & Dimension)
Standardized data for relationships and metrics:

- `fact_transaction`, `fact_review`
- `dim_customer`, `dim_product`, `dim_payment`, `dim_session`

### ğŸŸ¥ Datamart Layer (`marts_*`)
Aggregated tables for analytics use cases:

- `marts_customer_product_summary`
- `marts_payment_transaction_summary`
- `marts_review_product_summary`
- `marts_session_transaction_summary`

---

## ğŸ“Š EDA Summary (Before Transformation)

| Metric                  | Value          |
|--------------------------|----------------|
| Transactions             | ~16,500 rows   |
| Unique Products          | ~60,000 items  |
| Customers                | ~100,000       |
| Review Records           | ~16,500 rows   |
| Payment Types            | 12             |
| Web Sessions             | ~50,000 rows   |

---

## ğŸ§© Challenges Faced

- DAG scheduling and dependencies for multi-stage data movement
- Incremental and partitioned loading in BigQuery
- Exception handling for email alerts
- dbt backfill management

---

## ğŸš€ Future Improvements

- âœ… Add data validation with **dbt test** or **Great Expectations**
- ğŸ” Use **parameterized DAGs** and Jinja templating
- ğŸš€ Implement **CI/CD** for dbt and Airflow (GitHub Actions)
- ğŸ“ˆ Integrate basic **predictive ML models**
- â˜ï¸ Migrate Airflow to **Cloud Composer**
- ğŸ” Improve monitoring with **Stackdriver** or **Prometheus**

---

## âœ… Conclusion

This project delivers a structured and automated data pipeline for e-commerce analytics. From ingestion to visualization, the solution empowers decision-makers with near real-time business insights. The architecture is modular, scalable, and ready for production deployment or advanced analytics use cases.
