
# Ecommerce Data Pipeline and Analysis

This project implements a complete data engineering workflow for processing and analyzing e-commerce data using batch (ELT) and streaming pipelines. It automates data ingestion from PostgreSQL to Google BigQuery using Airflow, transforms data using dbt, and delivers insights via Looker Studio dashboards.

---

## ğŸ“‚ Configuration & Secrets

- Store your GCP credentials in: `config/`
- Airflow variables and SMTP secrets are stored securely via environment or Airflow UI.

---

## ğŸ¯ Objectives

- Build scheduled data pipelines using **Airflow + Docker**
- Automate data loading to **PostgreSQL** using Python
- Extract data from PostgreSQL â†’ **GCS**
- Load data from GCS â†’ **BigQuery**
- Transform data using **dbt** into multiple layers:
  - Staging
  - Dimension & Fact
  - Datamart
- Visualize insights using **Looker Studio**
- Enable real-time alerts via **SMTP Email Notification**

---

## ğŸ—ï¸ Project Structure

```bash
â”œâ”€â”€ airflow/                      
â”‚   â”œâ”€â”€ dags/                     # All Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ load_postgresql.py
â”‚   â”‚   â”œâ”€â”€ extract_postgres_to_gcs.py
â”‚   â”‚   â”œâ”€â”€ load_gcs_to_bq.py
â”‚   â”‚   â”œâ”€â”€ transform_dimfact.py
â”‚   â”‚   â””â”€â”€ transform_datamart.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dbt_script/                          
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ dim_table/
â”‚   â”‚   â”œâ”€â”€ fact_table/
â”‚   â”‚   â””â”€â”€ data_mart/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ script/
â”‚   â””â”€â”€ main.py         # Python script to insert dummy data into PostgreSQL
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ key.json                  # GCP service account key
â”‚
â”œâ”€â”€ datamaster/
â”‚   â””â”€â”€ customer.csv, etc
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## Architecture Overview

![Pipeline Architecture](./assets/pipeline_architecture.png)

---

## How to Run

### Batch (ELT) Pipeline

1. **Start Build Docker**
   ```bash
   docker-compose build
   ```

2. **Start Airflow with Docker**
   ```bash
   docker-compose up -d
   ```

3. **Trigger DAG**
   - Open Airflow UI at `http://localhost:8080`
   
   In Airflow UI, manually trigger the following DAGs sequentially:

   1. `load_postgresql` (optional if you run the script manually)
   2. `extract_postgres_to_gcs`
   3. `load_gcs_to_bq`
   4. `transform_dimfact`
   5. `transform_datamart`

---

### View Dashboard

Open your [Looker Studio](https://lookerstudio.google.com/reporting/d4e2a9ba-5e40-45eb-8298-30b6eebfa445) dashboard (linked manually).

Metrics visualized:

- Total Sales & Revenue Trends
- Top Selling Products
- Customer Transaction Behavior
- Payment Preferences
- Channel & Store Performance

---

### Email Notification

If any Airflow task fails, you will receive an email with:

- DAG and task names
- Error message and exception
- Execution timestamp
- Log URL (Airflow UI)

Configured via Airflow callbacks with SMTP (`send_email_smtp`).

---

## Data Warehouse Layers

### Staging Layer (`stg_*`)
Raw structured data from GCS â†’ BigQuery.

### Core Layer (Fact & Dimension)
Standardized data for relationships and metrics:

- `fact_transaction`, `fact_review`
- `dim_customer`, `dim_product`, `dim_payment`, `dim_session`

### Datamart Layer (`marts_*`)
Aggregated tables for analytics use cases:

- `marts_customer_product_summary`
- `marts_payment_transaction_summary`
- `marts_review_product_summary`
- `marts_session_transaction_summary`

---

## Future Improvements

- âœ… Add data validation with **dbt test** or **Great Expectations**
- ğŸ” Use **parameterized DAGs** and Jinja templating
- ğŸš€ Implement **CI/CD** for dbt and Airflow (GitHub Actions)
- ğŸ“ˆ Integrate basic **predictive ML models**
- â˜ï¸ Migrate Airflow to **Cloud Composer**
- ğŸ” Improve monitoring with **Stackdriver** or **Prometheus**

---

## Conclusion

This project delivers a structured and automated data pipeline for e-commerce analytics. From ingestion to visualization, the solution empowers decision-makers with near real-time business insights. The architecture is modular, scalable, and ready for production deployment or advanced analytics use cases.
